# The output and correctness of each algorithm

You should summarize, visualize, or highlight some part of the full-scale run of each algorithm. Additionally, the report should briefly describe what tests you performed to confirm that each algorithm was working as intended.

**A\*:** A* finds some reasonably short path between two movies, though as a result of our flawed heuristic (which is not admissible or consistent), this is often not the shortest path. However, using the heuristic tends to be significantly faster than if the heuristic was simply returned a constant, which is as expected. With regards to testing, we tested both small scale, hand designed datasets made to specifically test certain aspects, such as the effectiveness of the heuristic or its handling of not finding any path between two vertices, and tests using the whole dataset, which ensures a certain level of speed. The full scale run of A* prints to the console and saves the path from a given starting movie to the end movie, as well as the cumulative "distance" along this path.

**BFS**: The full-scale run of BFS writes to a file all of the films traversed from a breadth-first search from the starting movie to the destination movie. This gives the "neighborhood" around the two input movies. A relatively small BFS is one from "The Avengers" to "The Lion King" consisting of 198 films. Here we can see a lot of superhero movies, as one might expect, such as "Thor: The Dark World" right next to it and "Batman and Harley Quinn" closer to Lion King. The movies differ more along the BFS as is expected given that the traversal is spreading out further, with action film "Angel Has Fallen", drama "The Silent Child", and children's movie "Smallfoot" all next to each other near the end of the search.

The tests used the same small example graph to perform a BFS on from different starting locations. I counted the distance of each node from the starting node of each test case and checked that each element was the correct distance from the starting node.

**Prim:** The results outputted by Prim's algorithm show an interesting look into the way the algorithm functions within the limits of the dataset. Something that was appearent from the results was that there are certain "outlier" movies that are present in almost every instance of the algorithm being run. The most prominent example of this is the 1894 silent film "Edison Kinetoscopic Record of a Sneeze" appearing near the top of most of the lists that I tried during informal testing. The movie is a very short film of a man sneezing, and has seemingly no relevance to any movie it appears near on the outputted list. When the algorithm was tested with the movie "Moonrise Kingdom", it appeared as the 7th entry on the list. When ran with the 1953 Western "Shane", it is the 9th entry on the list. This is despite the fact that other movies on the lists appeared to show certain correlations that were not entirely arbitrary. For example, in the referenced test with the movie "Shane", there were several results that were either also westerns, were made around the same time period, or had very specific elements tying them to the other results. There were 3 movies on the output list of 60 that were about Samurai, and 6 that were European new wave films from the 1960s and 70s. Although it is possible that these types of films were somehow overrepresented in the data set, it points to the idea that there is some ability for Prim's algorithm to create connections between films in ways that would be coherent to a user.

# The answer to your leading question

You should direct address your proposed leading question. How did you answer this question? What did you discover? If your project was ultimately unsuccessful, give a brief reflection about what worked and what you would do differently as a team.

Our leading question was to create a graph that will aim to determine the similarity of different movies by connecting movies that have been reviewed similarly by different users. We answered this question by taking the average difference in rating by the same user between each movie in our graph. This allowed us to see how movies were rated differently by people with similar tastes. We saw that the output of our algorithms did not really follow along strict genre lines, which could be useful for providing recommendations that people may be originally unaware of. However, we suspect that our data set was too small, with too few reviews across too many movies to give a reliable measure of similarity. Another limitation of the dataset itself was the methods by which the data was collected. Because the data was collected through Twitter, reviews of older movies are underrepresented.

BFS answers the leading question by providing a variety of movies that were given ratings by the same people, with the ability to restrict how many movies you want to see by choosing movies that are closer or further away to the beginning to increase the spread of the traversal. This allows users to see a lot of movies and also could be helpful in challenging their ideas about what movies are grouped together.

If we were to do more work on this project, an interesting guiding question would be to look into the ways in which the algorithm worked, such as by being able to connect movies with similar plot elements, common actors, or a similar release year to a degree that does not just appear arbitrary to the average user, and identify the ways in the algorithm would be able to filter out random or arbitrary movies that are present in the results. For example, when performing an analysis of the output of Prim's algorithm when run to output 20 results with "Moonrise Kingdom" as the starting movie and determining if the descriptions of the movies shared common themes, 7 out of the 19 movies (other than "Moonrise Kingdom") referenced the theme/genre of "Coming of Age" or starred child actors. Although a small sample size and somewhat subjective criteria, the results suggested that if criteria was created to filter out movies that are dissimilar, the results could be greatly improved. Another improvement that is suggested by the results is making the output more user friendly. For example, because the name of the movies is the only information being stored by the outputs, there is no year, language, or other identifying information that could help someone find the movie. This also causes a great deal of confusion when there is more than one movie sharing the same name or multiple remmakes of the same movie. Because of this flaw, it is sometimes impossible to evaluate the quality of certain results from the output due to an inibility to correctly identify the movie. If more work was done on this project, these are important issues to consider for the future.
